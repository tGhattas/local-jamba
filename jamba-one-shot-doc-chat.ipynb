{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ce554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T08:58:59.195244Z",
     "iopub.status.busy": "2025-10-20T08:58:59.194701Z",
     "iopub.status.idle": "2025-10-20T08:59:00.340030Z",
     "shell.execute_reply": "2025-10-20T08:59:00.339189Z"
    },
    "papermill": {
     "duration": 1.149359,
     "end_time": "2025-10-20T08:59:00.341288",
     "exception": false,
     "start_time": "2025-10-20T08:58:59.191929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One-shot long-context Q&A and summary with AI21 Jamba Reasoning 3B (no RAG)\n",
    "import os\n",
    "import requests, os, pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_NAME = \"ai21labs/AI21-Jamba-Reasoning-3B\"\n",
    "GEN_MAX_NEW_TOKENS = 4096  # reserve generation budget\n",
    "\n",
    "print(\"Model:\", MODEL_NAME)\n",
    "\n",
    "PDF_URL = \"https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/09/key-findings-and-integration-strategies-on-the-impact-of-digital-technologies-on-students-learning_fad2ee0b/ab309c32-en.pdf\"   # <- put your link here\n",
    "PDF_PATH = \"/kaggle/working/doc.pdf\"\n",
    "\n",
    "with requests.get(PDF_URL, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(PDF_PATH, \"wb\") as f:\n",
    "        for chunk in r.iter_content(1024 * 64):\n",
    "            f.write(chunk)\n",
    "\n",
    "print(\"Saved:\", PDF_PATH, pathlib.Path(PDF_PATH).stat().st_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a8d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T08:59:00.347128Z",
     "iopub.status.busy": "2025-10-20T08:59:00.346719Z",
     "iopub.status.idle": "2025-10-20T08:59:30.614338Z",
     "shell.execute_reply": "2025-10-20T08:59:30.613280Z"
    },
    "papermill": {
     "duration": 30.273516,
     "end_time": "2025-10-20T08:59:30.617317",
     "exception": true,
     "start_time": "2025-10-20T08:59:00.343801",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load PDF as a single text blob using PyPDFLoader and merge pages\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "page_docs = loader.load()  # one Document per page\n",
    "print(f\"Loaded {len(page_docs)} pages\")\n",
    "\n",
    "full_text = \"\\n\\n\".join(d.page_content for d in page_docs)\n",
    "merged_doc = Document(page_content=full_text, metadata={\"source\": PDF_PATH})\n",
    "\n",
    "print(\"Merged characters:\", len(merged_doc.page_content))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a9978",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load tokenizer/model on GPU and report tokenizer/model limits\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is required to test long-context for Jamba\")\n",
    "\n",
    "# Load tokenizer first to inspect max length\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model_max_len = getattr(tokenizer, \"model_max_length\", None)\n",
    "print(\"Tokenizer model_max_length:\", model_max_len)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    ")\n",
    "model.to(\"cuda\")\n",
    "# Enable fast kernels if available\n",
    "if hasattr(model.config, \"use_mamba_kernels\"):\n",
    "    model.config.use_mamba_kernels = True\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded on:\", next(model.parameters()).device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4c6462",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper: apply chat template and generate without truncating the document\n",
    "from typing import List, Dict\n",
    "\n",
    "def build_chat_with_document(doc_text: str, user_query: str | None = None) -> List[Dict[str, str]]:\n",
    "    messages = []\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. You will receive the full document content in one message. \"\n",
    "        \"Answer questions or summarize based strictly on the provided content.\"\n",
    "    )\n",
    "    messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    if user_query is None:\n",
    "        user_content = (\n",
    "            \"Read the following document and be ready to answer questions about it.\\n\\n\" + doc_text\n",
    "        )\n",
    "    else:\n",
    "        user_content = (\n",
    "            \"Here is the full document context:\\n\\n\" + doc_text + \"\\n\\nQuestion: \" + user_query\n",
    "        )\n",
    "    messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(tokenizer(text).input_ids)\n",
    "\n",
    "\n",
    "def generate_with_long_context(messages: List[Dict[str, str]], max_new_tokens: int = GEN_MAX_NEW_TOKENS) -> str:\n",
    "    try:\n",
    "        formatted = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    except Exception:\n",
    "        # Fallback if chat template not provided\n",
    "        formatted = \"\\n\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in messages])\n",
    "\n",
    "    # Convert to tokens; we do NOT set truncation=True because we must not drop context\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\")\n",
    "\n",
    "    # Safety: if too long for the model, raise with actionable hint\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    limit = getattr(tokenizer, \"model_max_length\", None)\n",
    "    if limit is not None and input_len + max_new_tokens > limit:\n",
    "        raise ValueError(\n",
    "            f\"Prompt too long for model context window: input={input_len}, gen={max_new_tokens}, limit={limit}. \"\n",
    "            \"Reduce document size or increase context window (different model).\"\n",
    "        )\n",
    "\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdb07b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inspect token count of the merged document to ensure it fits\n",
    "merged_tokens = count_tokens(merged_doc.page_content)\n",
    "print({\n",
    "    \"merged_characters\": len(merged_doc.page_content),\n",
    "    \"merged_tokens\": merged_tokens,\n",
    "    \"tokenizer_model_max_length\": getattr(tokenizer, \"model_max_length\", None),\n",
    "    \"gen_tokens_budget\": GEN_MAX_NEW_TOKENS,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db6f9e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask(question: str, hide_thinking_trace: bool = True):\n",
    "    messages = build_chat_with_document(merged_doc.page_content, user_query=question)\n",
    "    answer = generate_with_long_context(messages, max_new_tokens=GEN_MAX_NEW_TOKENS)\n",
    "    print(\"Q:\", question)\n",
    "    print(\"A:\\n\", answer.rsplit(\"</think>\")[-1] if hide_thinking_trace else answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe7c38",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize(prompt: str = \"Summarize the following document concisely, preserving key metrics and conclusions\", hide_thinking_trace: bool = True):\n",
    "    # One-shot summary over the full document (no RAG)\n",
    "    messages_sum = build_chat_with_document(merged_doc.page_content, user_query=None)\n",
    "    # Replace the last user content to explicitly ask for a concise summary\n",
    "    messages_sum[-1][\"content\"] = (\n",
    "        f\"{prompt}\\n\\n\"\n",
    "        + merged_doc.page_content\n",
    "    )\n",
    "    summary = generate_with_long_context(messages_sum, max_new_tokens=GEN_MAX_NEW_TOKENS)\n",
    "    summary = summary.rsplit(\"</think>\")[-1] if hide_thinking_trace else summary\n",
    "    print(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfafa420",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Q&A and Document Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5659f2dd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ask(\"What chapter is devoted to cyberbullying, and what types of interventions does it recommend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e96a5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ask(\"List all the tables and figures included in the report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa8cf9b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ask('Which table mentions the challenge of \"digital equity\" and what example studies does it cite as evidence for this challenge?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e81965",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarize()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3717.860408,
   "end_time": "2025-10-20T08:59:34.226961",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-20T07:57:36.366553",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
